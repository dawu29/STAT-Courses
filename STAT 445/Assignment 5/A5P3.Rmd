---
title: "A5P3"
output: pdf_document
author: Dawu Liu
---
In this assignment, I will write  principal component as \textbf{PC} sometimes for short.
```{r, echo=FALSE, results='hide', message=FALSE,warning = FALSE}
library(readxl)
library(factoextra)
X <- read_excel("C:/Users/John/Desktop/STAT 445/Data/malifarmdata.xlsx")
X
```

\textbf{(a)} \

\
i. Matrix scatter plot of the data

```{r, echo = FALSE}
pairs(X,pch=20, col = 2, oma = c(0,0,0,0),cex=0.8)
```

\newpage
ii. Boxplot of the data

```{r, echo=FALSE}
par(cex.axis=0.8)
boxplot(X, main = "box plot of Mali Farm data")
```
\
iii.

```{r, echo=FALSE,fig.height=4, fig.width=7}
S <- cov(X)
x_bar <- colMeans(X)
D <- c() 
for (i in 1:nrow(X)) {
  x_i <- t(X[i,])
  D[i]=t(x_i-x_bar)%*%solve(S)%*%(x_i-x_bar)
}
plot(D, main = "D values vs index", pch=20);text(D, pos = 2, offset = 0.3)
```

\
iv. I added qq-plot to help detect the outliers. 

```{r, echo=FALSE,fig.height=8, fig.width=8}
par(mfrow=c(3, 3),mar= c(2,2,2,2), oma=c(0,0,0,0))
q <- vector("list", length = 9)
for (i in 1:9) {
  q[[i]]=qqnorm(X[[i]],type="n", main = names(X)[i]);qqline(X[[i]],col="red")
  text(q[[i]]$x, q[[i]]$y)
}
```
\newpage
Also three scatter plots:

```{r, echo = FALSE, fig.height = 3, fig.width = 9}
par(mfrow=c(1,3))
plot(X[[5]],X[[4]],type="n",xlab=names(X)[5],ylab=names(X)[4])
text(X[[5]],X[[4]])
plot(X[[3]],X[[8]],type="n",xlab=names(X)[3],ylab=names(X)[8])
text(X[[3]],X[[8]])
plot(X[[2]],X[[3]],type="n",xlab=names(X)[2],ylab=names(X)[3])
text(X[[2]],X[[3]])
```
The outliers are 25, 34, 52, 57, 62, 69, 72. 

From the three scatter plots above, Maize vs Sorg indicates 25; Cattle vs Cotton indicates 34; and 
Cotton vs DistRD indicates 69, 72 are the outliers.

All 7 outliers have relatively high values in the distance values plot. 

In both the Boxplot and Q-Q plot, Sorg indicates 25; Cattle indicates 34; Cotton indicates 52, 57; Maize indicates 62; 
and DistRD indicates 69 and 72 are the outliers.

The table below shows which indicator indicates the outliers.

```{r echo=FALSE}
dtable <- data.frame(matrix(ncol = 7, nrow = 4))
colnames(dtable) <- c(25,34,52,57,62,69,72)
rownames(dtable) <- c("Scatter Plots", "Distance Plot", "Box Plot", "Q-Q Plots")
dtable[1,] <- c("yes","yes"," "," "," ","yes","yes")
dtable[2,] <- c("yes","yes","yes","yes","yes","yes","yes")
dtable[3,] <- c("yes","yes","yes","yes","yes","yes","yes")
dtable[4,] <- c("yes","yes","yes","yes","yes","yes","yes")
dtable
Y <- X[-c(25,34,52,57,62,69,72),]
```
\
v. Create a data matrix $\tilde{X}$ by removing the outliers.

New data set $\tilde{X}$ is created by removing row 25,34,52,57,62,69,72 and 
will be used in the second PCA of part (b).\

analyses starts on the next page...

\newpage
\textbf{(b)}

\textbf{PCA on the original data set X}
```{r, echo=FALSE}
X_pr = prcomp(X, center = TRUE, scale. = FALSE)
X_eigen_table = get_eigenvalue(X_pr)
```
\
i. Sample covariance matrix \textbf{S}
```{r echo=FALSE}
S <- cov(X)
round(S,3)
```
\
ii. The eigenvalues are:
```{r, echo = FALSE}
round(X_eigen_table[,1], 4)
```
The first eigenvalue accounts for the majority of the total variance. PC4 to PC9 have 
very little on the proportion of the total variance. (table shown in iii)

\
iii.

Criteria 1, eigenvalues and their cumulative proportions table
```{r, echo = FALSE}
round(X_eigen_table,4)
```
This method suggests to keep \textbf{1} principal component which gives 89.5% of the total variance.\

Criteria 2, check which eigenvalue(s) is greater than the mean of eigenvalues

The mean of the eigenvalues is:
```{r, echo = FALSE}
mean(X_eigen_table[,1])
```
This method suggests to keep \textbf{1} principal component. \

Criteria3, scree plot

```{r,echo = FALSE,fig.height=4, fig.width=4}
plot(X_eigen_table[,1], type = "o", pch = 15, main = "Scree Plot",
     xlab = "Principal Component Number", ylab = "Eigenvalues")
```

The "bend" occurs at PC2, indicating from PC2 and on, the the eigenvalues are relatively small. 
This method suggests to keep \textbf{1} principal component. \

Overall, the criteria suggest \textbf{1} principal component should be retained.

But in order to make a scatter plot, we will use 2 principal components.\

\
iv. The eigenvectors for the principal components:

```{r, echo = FALSE}
e_X_1 <- X_pr$rotation[,1]
e_X_2 <- X_pr$rotation[,2]
```
$(\tilde{\hat{e_1}})^T$ = 
```{r, echo = FALSE}
round(e_X_1, 4)
```

$(\tilde{\hat{e_2}})^T$ = 
```{r, echo = FALSE}
round(e_X_2, 4)
```
\
v.

PC1 almost entirely depends on DistRD, it's a component representing DistRD. PC2 almost entirely depends on Family, 
with some dependency on cattle. For both PCs, all the crops and farm animals have very little loads on them.\

\
vi. Scatter plot for PC2 vs PC1

```{r,echo = FALSE,fig.height=5, fig.width=5}
plot(X_pr$x[,1], X_pr$x[,2], xlab = "PC1 (89.53%)", ylab = "PC2 (8.08%)", pch = 20, col = 2)
text(X_pr$x[,1], X_pr$x[,2], pos = 2, offset = 0.3, cex = 0.5)
abline(v = 0, h = 0)
```

Observation number 69 and 72 on the far left have extreme small PC1 values compare to the rest of the data, means they are outliers with 
larges values in DistRD(negative coefficients). Based on this plot, it appears most of the observations are grouped together in terms of 
PC1 values, but this might be a scaling issue caused by the outliers. \


\textbf{PCA on the new data set $\tilde{X}$ where the outliers are removed}
```{r, echo=FALSE}
Y_pr = prcomp(Y, center = TRUE, scale. = FALSE)
Y_eigen_table = get_eigenvalue(Y_pr)
```
\
i. Sample covariance matrix \textbf{S}
```{r echo=FALSE}
Sy <- cov(Y)
round(Sy,3)
```
\
ii. The eigenvalues are:
```{r, echo = FALSE}
round(Y_eigen_table[,1], 4)
```
The first and second eigenvalues account for the majority of the total variance. PC3 to PC9 have 
very little on the proportion of the total variance. (table shown in iii)

\
iii.

Criteria 1, eigenvalues and their cumulative proportions table
```{r, echo = FALSE}
round(Y_eigen_table,4)
```
This method suggests to keep \textbf{2} principal component which gives 95.2% of the total variance.\

Criteria 2, check which eigenvalue(s) is greater than the mean of eigenvalues

The mean of the eigenvalues is:
```{r, echo = FALSE}
mean(Y_eigen_table[,1])
```
This method suggests to keep \textbf{2} principal component. \

Criteria3, scree plot

```{r,echo = FALSE,fig.height=4, fig.width=6}
plot(Y_eigen_table[,1], type = "o", pch = 15, main = "Scree Plot",
     xlab = "Principal Component Number", ylab = "Eigenvalues")
```

The "bend" occurs at PC3, indicating from PC3 and on, the the eigenvalues are relatively small. 
This method suggests to keep \textbf{2} principal component. \

Overall, all three criteria suggest \textbf{2} principal components should be retained.

\
iv. The eigenvectors for the principal components:

```{r, echo = FALSE}
e_Y_1 <- Y_pr$rotation[,1]
e_Y_2 <- Y_pr$rotation[,2]
```
$(\tilde{\hat{e_1}})^T$ = 
```{r, echo = FALSE}
round(e_Y_1, 4)
```

$(\tilde{\hat{e_2}})^T$ = 
```{r, echo = FALSE}
round(e_Y_2, 4)
```

\
v.
Similar to before, PC1 almost entirely depends on DistRD, with rest of the variables having loads about 
equal to zero. PC2 almost entirely depends on family, and some dependency on cattle. 
For both PCs, all the crops and farm animals have very little loads on them.

\
vi. Scatter plot for PC2 vs PC1

```{r,echo = FALSE,fig.height=5, fig.width=6}
plot(Y_pr$x[,1], Y_pr$x[,2], xlab = "PC1 (60.96%)", ylab = "PC2 (34.23%)", pch = 20, col = 2)
text(Y_pr$x[,1], Y_pr$x[,2], pos = 2, offset = 0.3, cex = 0.5)
abline(v = 0, h = 0)
```


In the content, DistRD appears to be the distance to the road. There is a cluster on the left button 
area with relatively small PC1 and PC2 values, showing a significant portion of farms 
with fewer family members tend to live closer to the road. Farm 1, 6, 66, 67, and 68 on the far right shows 
farms are far away from the road tend to have relatively low family members. Also, there is a gap along the 
vertical axis.\

\textbf{(c) Comparaison on the results for the two analyses}

Removing the outliers has relatively little effect on the eigenvectors(coefficients) for the first and second principal 
components, i.e. what they represent. But it significantly increases the weighing(eigenvalue) of PC2 on the total variance, 
which was overshadowed by PC1 before removing the outliers. Therefore, the second test suggests to keep two 
principal components instead of one in the first test. 
It also helps with scaling issue on the scatter plot, allows us to obtain information from it clearly.

I like the result of the analysis after removing the outliers better, because it gives us more clear images on the 
analysis, while removing very little information from it. Also, it shows some information we might 
ignore before.

P.S. Due to the scaling issue of Family and DistRD having dominating values, 
we were not able to obtain much information on the crops and farm animals from both analyses. 
I looked at the eigenvectors for PC3 to PC9, 5 out 6 are dominated by only one coefficient, meaning we can't
obtain relationships between the crops and farm animals from them either. 
Running PCA using sample correlation matrix R might be a better choice, 
but we might end up choosing more PCs. \

\
Code used to solve the questions(graphs are hidden):
```{r fig.show='hide'}
rm(list = ls())
library(readxl)
library(factoextra)
X <- read_excel("C:/Users/John/Desktop/STAT 445/Data/malifarmdata.xlsx")

# a
pairs(X,pch=20, col = 2, oma = c(0,0,0,0),cex=0.8)
par(cex.axis=0.8)
boxplot(X, main = "box plot of Mali Farm data")
S <- cov(X)
x_bar <- colMeans(X)
D <- c() 
for (i in 1:nrow(X)) {
  x_i <- t(X[i,])
  D[i]=t(x_i-x_bar)%*%solve(S)%*%(x_i-x_bar)
}
plot(D, main = "D values vs index", pch=20);text(D, pos = 2, offset = 0.3)
par(mfrow=c(3, 3),mar= c(2,2,2,2), oma=c(0,0,0,0))
q <- vector("list", length = 9)
for (i in 1:9) {
  q[[i]]=qqnorm(X[[i]],type="n", main = names(X)[i]);qqline(X[[i]],col="red")
  text(q[[i]]$x, q[[i]]$y)
}
dtable <- data.frame(matrix(ncol = 7, nrow = 4))
colnames(dtable) <- c(25,34,52,57,62,69,72)
rownames(dtable) <- c("Scatter Plots", "Q-Q Plots", "Distance Plot", "Box Plot")
dtable[1,] <- c("yes","yes"," "," "," ","yes","yes")
dtable[2,] <- c("yes","yes","yes","yes","yes","yes","yes")
dtable[3,] <- c("yes","yes","yes","yes","yes","yes","yes")
dtable[4,] <- c("yes","yes","yes","yes","yes","yes","yes")
dtable
Y <- X[-c(25,34,52,57,62,69,72),]

# b 
# analysis 1
X_pr = prcomp(X, center = TRUE, scale. = FALSE)
X_eigen_table = get_eigenvalue(X_pr)
S <- cov(X)
round(S,3)
round(X_eigen_table[,1], 4)
round(X_eigen_table,4)
mean(X_eigen_table[,1])
plot(X_eigen_table[,1], type = "o", pch = 15, main = "Scree Plot",
     xlab = "Principal Component Number", ylab = "Eigenvalues")
e_X_1 <- X_pr$rotation[,1]
e_X_2 <- X_pr$rotation[,2]
round(e_X_1, 4)
round(e_X_2, 4)
plot(X_pr$x[,1], X_pr$x[,2], xlab = "PC1 (89.53%)", ylab = "PC2 (8.08%)", pch = 20, col = 2)
text(X_pr$x[,1], X_pr$x[,2], pos = 2, offset = 0.3, cex = 0.5)
abline(v = 0, h = 0)

# analysis 2
Y_pr = prcomp(Y, center = TRUE, scale. = FALSE)
Y_eigen_table = get_eigenvalue(Y_pr)
Sy <- cov(Y)
round(Sy,3)
round(Y_eigen_table[,1], 4)
round(Y_eigen_table,4)
mean(Y_eigen_table[,1])
plot(Y_eigen_table[,1], type = "o", pch = 15, main = "Scree Plot",
     xlab = "Principal Component Number", ylab = "Eigenvalues")
e_Y_1 <- Y_pr$rotation[,1]
e_Y_2 <- Y_pr$rotation[,2]
round(e_Y_1, 4)
round(e_Y_2, 4)
plot(Y_pr$x[,1], Y_pr$x[,2], xlab = "PC1 (60.96%)", ylab = "PC2 (34.23%)", pch = 20, col = 2)
text(Y_pr$x[,1], Y_pr$x[,2], pos = 2, offset = 0.3, cex = 0.5)
abline(v = 0, h = 0)
```













